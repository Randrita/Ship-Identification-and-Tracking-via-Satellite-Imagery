{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n<p>\n<font size=\"9\" face=\"Arial\">\n<h1 align=\"center\"> Ship-Identification-and-Tracking-via-Satellite-Imagery\n </h1>\n</font>\n</p>\n\n\n--------------","metadata":{}},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">In this notebook, I will create a model that predicts weather there is ship in the image or not.</font>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p>\n<font size=\"6\" face=\"Arial\">\n<h2 align=\"left\"> 1. Dataset Information </h2> \n</font>\n</p>\n<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Context </h2>\n</font>\n</p>\n\n    \n<p>\n<font size=\"4\" face=\"Arial\">\nSatellite imagery provides unique insights into various markets, including agriculture, defense and intelligence, energy, and finance. New commercial imagery providers, such as Planet, are using constellations of small satellites to capture images of the entire Earth every day.\n\nThe aim of this dataset is to help address the difficult task of detecting the location of large ships in satellite images. Automating this process can be applied to many issues including monitoring port activity levels and supply chain analysis.\n</font>\n</p>\n</br>\n<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Content </h2>\n</font>\n<p>\n<font size=\"4\" face=\"Arial\">\nThe dataset consists of image chips extracted from Planet satellite imagery collected over the San Francisco Bay and San Pedro Bay areas of California. It includes 4000 80x80 RGB images labeled with either a \"ship\" or \"no-ship\" classification. Image chips were derived from PlanetScope full-frame visual scene products, which are orthorectified to a 3 meter pixel size.\n</font>\n</p>\n</br>   \n\n\n\n\n<p>\n<font size=\"4\" face=\"Arial\">    \nThe dataset is also distributed as a JSON formatted text file shipsnet.json. The loaded object contains data, label, scene_ids, and location lists.\n<ol type=\"square\">\n<li>**label**: Valued 1 or 0, representing the \"ship\" class and \"no-ship\" class, respectively.</li>\n<li>**scene id**: The unique identifier of the PlanetScope visual scene the image chip was extracted from. The scene id can be used with the Planet API to discover and download the entire scene.</li>\n<li>**longitude_latitude**: The longitude and latitude coordinates of the image center point, with values separated by a single underscore.</li>    \n</ol>    \nThe pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. The image is stored in row-major order, so that the first 80 entries of the array are the red channel values of the first row of the image.\n</font>\n</p>\n<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Class Labels </h2>\n</font>\n</p>\n\n<p>\n<font size=\"4\" face=\"Arial\">\nThe \"ship\" class includes 1000 images. Images in this class are near-centered on the body of a single ship. Ships of different sizes, orientations, and atmospheric collection conditions are included. Example images from this class are shown below.\n</font>  \n</p>\n\n<figure class=\"half\" style=\"center\">\n    <img style=\"width:700px\" src=\"https://i.imgur.com/tLsSoTz.png\">\n</figure>\n<p>\n<font size=\"4\" face=\"Arial\">\nThe \"no-ship\" class includes 3000 images. A third of these are a random sampling of different landcover features - water, vegetion, bare earth, buildings, etc. - that do not include any portion of an ship. The next third are \"partial ships\" that contain only a portion of an ship, but not enough to meet the full definition of the \"ship\" class. The last third are images that have previously been mislabeled by machine learning models, typically caused by bright pixels or strong linear features. Example images from this class are shown below.\n</font>\n</p>\n<figure class=\"half\" style=\"center\">\n    <img style=\"width:700px\" src=\"https://i.imgur.com/Q3daQMC.png\">\n</figure>","metadata":{}},{"cell_type":"markdown","source":"\n<p>\n<font size=\"6\" face=\"Arial\">\n<h2 align=\"left\"> 2. Loading and Checking Dataset </h2> \n</font>\n</p>\n   \n","metadata":{"execution":{"iopub.status.busy":"2021-08-10T12:42:11.886594Z","iopub.execute_input":"2021-08-10T12:42:11.88702Z","iopub.status.idle":"2021-08-10T12:42:11.892337Z","shell.execute_reply.started":"2021-08-10T12:42:11.886986Z","shell.execute_reply":"2021-08-10T12:42:11.891159Z"}}},{"cell_type":"markdown","source":"\n<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Loading and Reshaping </h2>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom numpy import expand_dims\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix\nfrom keras.utils import to_categorical\nimport keras\nfrom keras import layers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import EarlyStopping","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-23T13:23:41.745899Z","iopub.execute_input":"2022-03-23T13:23:41.746598Z","iopub.status.idle":"2022-03-23T13:23:47.997506Z","shell.execute_reply.started":"2022-03-23T13:23:41.746503Z","shell.execute_reply":"2022-03-23T13:23:47.996455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('../input/ships-in-satellite-imagery/shipsnet.json') as data_file:\n    dataset = json.load(data_file)\nshipsnet= pd.DataFrame(dataset)\nshipsnet.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:23:48.001479Z","iopub.execute_input":"2022-03-23T13:23:48.001796Z","iopub.status.idle":"2022-03-23T13:24:08.291275Z","shell.execute_reply.started":"2022-03-23T13:23:48.001766Z","shell.execute_reply":"2022-03-23T13:24:08.290163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shipsnet.info()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:08.293404Z","iopub.execute_input":"2022-03-23T13:24:08.293928Z","iopub.status.idle":"2022-03-23T13:24:08.315537Z","shell.execute_reply.started":"2022-03-23T13:24:08.293864Z","shell.execute_reply":"2022-03-23T13:24:08.314459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> We need just two columns in this project which are data and labels. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"shipsnet = shipsnet[[\"data\", \"labels\"]]\nshipsnet.head()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:08.317511Z","iopub.execute_input":"2022-03-23T13:24:08.317843Z","iopub.status.idle":"2022-03-23T13:24:08.343116Z","shell.execute_reply.started":"2022-03-23T13:24:08.317812Z","shell.execute_reply":"2022-03-23T13:24:08.342351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(shipsnet[\"data\"].iloc[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:08.344631Z","iopub.execute_input":"2022-03-23T13:24:08.345236Z","iopub.status.idle":"2022-03-23T13:24:08.354072Z","shell.execute_reply.started":"2022-03-23T13:24:08.34518Z","shell.execute_reply":"2022-03-23T13:24:08.353109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" <p>\n<font size=\"4\" face=\"Arial\">\n<li> As you can see, the pixel value data for each 80x80 RGB image is stored as a list of 19200 integers within the data list. The first 6400 entries contain the red channel values, the next 6400 the green, and the final 6400 the blue. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"ship_images = shipsnet[\"labels\"].value_counts()[0]\nno_ship_images = shipsnet[\"labels\"].value_counts()[1]\nprint(\"Number of the ship_images :{}\".format(ship_images),\"\\n\")\nprint(\"Number of the ship_images :{}\".format(no_ship_images))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:08.355147Z","iopub.execute_input":"2022-03-23T13:24:08.35557Z","iopub.status.idle":"2022-03-23T13:24:08.3679Z","shell.execute_reply.started":"2022-03-23T13:24:08.355531Z","shell.execute_reply":"2022-03-23T13:24:08.366926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Turning the json information into numpy array and then assign it as x and y variables\nx = np.array(dataset['data']).astype('uint8')\ny = np.array(dataset['labels']).astype('uint8')","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:08.36898Z","iopub.execute_input":"2022-03-23T13:24:08.369453Z","iopub.status.idle":"2022-03-23T13:24:23.641802Z","shell.execute_reply.started":"2022-03-23T13:24:08.369422Z","shell.execute_reply":"2022-03-23T13:24:23.640982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.643847Z","iopub.execute_input":"2022-03-23T13:24:23.644294Z","iopub.status.idle":"2022-03-23T13:24:23.650389Z","shell.execute_reply.started":"2022-03-23T13:24:23.644256Z","shell.execute_reply":"2022-03-23T13:24:23.649465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> The current data for each image is one row of flattened 19200 data points representing the RGB values of each pixel. So we need to reshape it. After reshaping, each item in new x variable will be 3 lists. Each of these lists will be RGB values for each pixel for the length and width of the image. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"x_reshaped = x.reshape([-1, 3, 80, 80])\n\nx_reshaped.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.652248Z","iopub.execute_input":"2022-03-23T13:24:23.652615Z","iopub.status.idle":"2022-03-23T13:24:23.663982Z","shell.execute_reply.started":"2022-03-23T13:24:23.652584Z","shell.execute_reply":"2022-03-23T13:24:23.663056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> We need to change the order of the dimensions to get the correct format to plot the images. The last value of the shape of x_reshaped will be 3, this number represent the channel. (RGB) </li>\n</font>\n</p>    ","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://e2eml.school/images/image_processing/three_d_array.png\" class=\"center\" style=\"width:400px\">\n\n","metadata":{}},{"cell_type":"code","source":"x_reshaped = x.reshape([-1, 3, 80, 80]).transpose([0,2,3,1])\nx_reshaped.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.665085Z","iopub.execute_input":"2022-03-23T13:24:23.665482Z","iopub.status.idle":"2022-03-23T13:24:23.678707Z","shell.execute_reply.started":"2022-03-23T13:24:23.665454Z","shell.execute_reply":"2022-03-23T13:24:23.677779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.680141Z","iopub.execute_input":"2022-03-23T13:24:23.680544Z","iopub.status.idle":"2022-03-23T13:24:23.69295Z","shell.execute_reply.started":"2022-03-23T13:24:23.680501Z","shell.execute_reply":"2022-03-23T13:24:23.691848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> y variable consist of label values, 1 or 0. We need to convert them binary class matrix. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"y_reshaped = to_categorical(y, num_classes=2)\n\ny_reshaped.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.694337Z","iopub.execute_input":"2022-03-23T13:24:23.694692Z","iopub.status.idle":"2022-03-23T13:24:23.707816Z","shell.execute_reply.started":"2022-03-23T13:24:23.69466Z","shell.execute_reply":"2022-03-23T13:24:23.706738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_reshaped","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.709127Z","iopub.execute_input":"2022-03-23T13:24:23.709422Z","iopub.status.idle":"2022-03-23T13:24:23.72212Z","shell.execute_reply.started":"2022-03-23T13:24:23.709396Z","shell.execute_reply":"2022-03-23T13:24:23.721004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Exploring the Images </h2>\n</font>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> A quick look at images with and without ships. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"image_no_ship = x_reshaped[y==0]\nimage_ship = x_reshaped[y==1]\n\ndef plot(a,b):\n    \n    plt.figure(figsize=(15, 15))\n    for i, k in enumerate(range(1,9)):\n        if i < 4:\n            plt.subplot(2,4,k)\n            plt.title('Not A Ship')\n            plt.imshow(image_no_ship[i+2])\n            plt.axis(\"off\")\n        else:\n            plt.subplot(2,4,k)\n            plt.title('Ship')\n            plt.imshow(image_ship[i+15])\n            plt.axis(\"off\")\n            \n    plt.subplots_adjust(bottom=0.3, top=0.7, hspace=0.25)\n\n#Implementation of the function \n\nplot(image_no_ship, image_ship)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:23.723556Z","iopub.execute_input":"2022-03-23T13:24:23.723889Z","iopub.status.idle":"2022-03-23T13:24:25.068594Z","shell.execute_reply.started":"2022-03-23T13:24:23.723859Z","shell.execute_reply":"2022-03-23T13:24:25.067799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> A quick look at pixel intensity of images. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"def plotHistogram(ship, not_ship):\n\n    plt.figure(figsize = (10,7))\n    plt.subplot(2,2,1)\n    plt.imshow(ship)\n    plt.axis('off')\n    plt.title('Ship')\n    histo = plt.subplot(2,2,2)\n    histo.set_ylabel('Count', fontweight = \"bold\")\n    histo.set_xlabel('Pixel Intensity', fontweight = \"bold\")\n    n_bins = 30\n    plt.hist(ship[:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\n    plt.hist(ship[:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\n    plt.hist(ship[:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\n    plt.show()\n    print(\"Minimum pixel value of this image: {}\".format(ship.min()))\n    print(\"Maximum pixel value of this image: {}\".format(ship.max()))\n    plt.figure(figsize = (10,7))\n    plt.subplot(2,2,3)\n    plt.imshow(not_ship)\n    plt.axis('off')\n    plt.title('Not A Ship')\n    histo = plt.subplot(2,2,4)\n    histo.set_ylabel('Count', fontweight = \"bold\")\n    histo.set_xlabel('Pixel Intensity', fontweight = \"bold\")\n    n_bins = 30\n    plt.hist(not_ship[:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\n    plt.hist(not_ship[:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\n    plt.hist(not_ship[:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\n    plt.show()\n    print(\"Minimum pixel value of this image: {}\".format(not_ship.min()))\n    print(\"Maximum pixel value of this image: {}\".format(not_ship.max()))\n#Implementation of the function\n\nfor i in range (20,23):\n    plotHistogram(x_reshaped[y==1][i], x_reshaped[y==0][i])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:25.069725Z","iopub.execute_input":"2022-03-23T13:24:25.070186Z","iopub.status.idle":"2022-03-23T13:24:27.49652Z","shell.execute_reply.started":"2022-03-23T13:24:25.070153Z","shell.execute_reply":"2022-03-23T13:24:27.495522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now let’s take a quick view of each channels in the two image. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"my_list = [(0, 'R channel'), (1, 'G channel'), (2, 'B channel')]\n\nplt.figure(figsize = (15,15))\n\nfor i, k in my_list:\n    plt.subplot(1,3,i+1)\n    plt.title(k)\n    plt.ylabel('Height {}'.format(x_reshaped[y==0][5].shape[0]))\n    plt.xlabel('Width {}'.format(x_reshaped[y==0][5].shape[1]))\n    plt.imshow(x_reshaped[y==0][5][ : , : , i])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:27.49798Z","iopub.execute_input":"2022-03-23T13:24:27.498377Z","iopub.status.idle":"2022-03-23T13:24:28.173321Z","shell.execute_reply.started":"2022-03-23T13:24:27.498343Z","shell.execute_reply":"2022-03-23T13:24:28.171976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"my_list = [(0, 'R channel'), (1, 'G channel'), (2, 'B channel')]\n\nplt.figure(figsize = (15,15))\n\nfor i, k in my_list:\n    plt.subplot(1,3,i+1)\n    plt.title(k)\n    plt.ylabel('Height {}'.format(x_reshaped[y==0][5].shape[0]))\n    plt.xlabel('Width {}'.format(x_reshaped[y==0][5].shape[1]))\n    plt.imshow(x_reshaped[y==1][5][ : , : , i])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:28.175081Z","iopub.execute_input":"2022-03-23T13:24:28.175516Z","iopub.status.idle":"2022-03-23T13:24:29.400161Z","shell.execute_reply.started":"2022-03-23T13:24:28.175475Z","shell.execute_reply":"2022-03-23T13:24:29.399116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"6\" face=\"Arial\">\n<h2 align=\"left\"> 3. Modelling </h2> \n</font>\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Preparing of Train and Test Data </h2>\n</font>\n</p>","metadata":{}},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> First step is normalizing x data. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"x_reshaped = x_reshaped / 255","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:29.401353Z","iopub.execute_input":"2022-03-23T13:24:29.401632Z","iopub.status.idle":"2022-03-23T13:24:29.623263Z","shell.execute_reply.started":"2022-03-23T13:24:29.401606Z","shell.execute_reply":"2022-03-23T13:24:29.622281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_reshaped[0][0][0] # Normalized RGB values of the firs pixel of the first image in the dataset.","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:29.624587Z","iopub.execute_input":"2022-03-23T13:24:29.624867Z","iopub.status.idle":"2022-03-23T13:24:29.631711Z","shell.execute_reply.started":"2022-03-23T13:24:29.624841Z","shell.execute_reply":"2022-03-23T13:24:29.630737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_bins = 30\nplt.hist(x_reshaped[y == 0][0][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\nplt.hist(x_reshaped[y == 0][0][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\nplt.hist(x_reshaped[y == 0][0][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\nplt.ylabel('Count', fontweight = \"bold\")\nplt.xlabel('Pixel Intensity', fontweight = \"bold\")\nplt.title(\"Histogram of normalized data\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:29.633497Z","iopub.execute_input":"2022-03-23T13:24:29.633915Z","iopub.status.idle":"2022-03-23T13:24:30.411758Z","shell.execute_reply.started":"2022-03-23T13:24:29.633864Z","shell.execute_reply":"2022-03-23T13:24:30.410731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Data is ready for splitting as train and test. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"x_train_1, x_test, y_train_1, y_test = train_test_split(x_reshaped, y_reshaped,\n                                                        test_size = 0.20, random_state = 42)\n\n\nx_train, x_val, y_train, y_val = train_test_split(x_train_1, y_train_1, \n                                                  test_size = 0.25, random_state = 42)\n\n\nprint(\"x_train shape\",x_train.shape)\nprint(\"x_test shape\",x_test.shape)\nprint(\"y_train shape\",y_train.shape)\nprint(\"y_test shape\",y_test.shape)\nprint(\"y_train shape\",x_val.shape)\nprint(\"y_test shape\",y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:30.413232Z","iopub.execute_input":"2022-03-23T13:24:30.4136Z","iopub.status.idle":"2022-03-23T13:24:30.820132Z","shell.execute_reply.started":"2022-03-23T13:24:30.41357Z","shell.execute_reply":"2022-03-23T13:24:30.819152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:30.821443Z","iopub.execute_input":"2022-03-23T13:24:30.821724Z","iopub.status.idle":"2022-03-23T13:24:30.828005Z","shell.execute_reply.started":"2022-03-23T13:24:30.821697Z","shell.execute_reply":"2022-03-23T13:24:30.826932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Implementation of Artificial Neural Network (ANN) </h2>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"from keras import callbacks\nmodel = Sequential()\nmodel.add(Flatten(input_shape=[80, 80, 3]))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(150, activation='relu'))\nmodel.add(Dense(2, activation='sigmoid'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nearlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", patience = 10, \n                                        restore_best_weights = True)\n\nhistory = model.fit(x_train, y_train, epochs = 100, validation_data=(x_val, y_val), callbacks = [earlystopping])","metadata":{"execution":{"iopub.status.busy":"2022-03-23T13:24:30.833238Z","iopub.execute_input":"2022-03-23T13:24:30.833524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now I will add two more layer and change activation function as softmax . </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"from keras import callbacks\nmodel = Sequential()\nmodel.add(Flatten(input_shape=[80, 80, 3]))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nearlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", patience = 10, \n                                        restore_best_weights = True)\n\nhistory = model.fit(x_train, y_train, epochs = 100, validation_data=(x_val, y_val), callbacks = [earlystopping])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now we can predict our test data with ANN. I will check the first image of the test data. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(x_test)\npd.Series(prediction[0], index=[\"Not A Ship\", \"Ship\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x_test[0])\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Let's take a look at the images that are estimated to be ship, although not a ship. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"predicted_data = pd.DataFrame(prediction, columns=[\"Not A Ship\", \"Ship\"])\npredicted_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_data = pd.DataFrame(y_test, columns=[\"Not A Ship\", \"Ship\"])\ny_test_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_data['There is a Ship'] = y_test[:, 1]\npredicted_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_data[\"Difference\"] = predicted_data[\"Ship\"] - predicted_data[\"There is a Ship\"]\npredicted_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> If difference is high, that means the image predicted as Ship, altought not a ship. To see such predicted images, we need to sort the difference column from largest to smallest. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"predicted_data.sort_values('Difference', ascending=False).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now, I want to show some of these images and their pixel intensity histograms. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"indexes = predicted_data.sort_values('Difference', ascending = False).head(4).index.to_list()\n\ndef plotHistogram(image_index):\n\n    plt.figure(figsize = (10,7))\n    plt.subplot(2,2,1)\n    plt.imshow(x_test[image_index])\n    plt.axis('off')\n    plt.title('There is no ship. But predicted as a ship.')\n    histo = plt.subplot(2,2,2)\n    histo.set_ylabel('Count', fontweight = \"bold\")\n    histo.set_xlabel('Pixel Intensity', fontweight = \"bold\")\n    n_bins = 30\n    plt.hist(x_test[image_index][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\n    plt.show()\n\n\n#Implementation of the function\n\nfor i in indexes:\n    plotHistogram(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now, let's take a look at the images that are estimated to not a ship, although there is a ship. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"predicted_data.sort_values('Difference', ascending=True).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now, I want to show some of these images and their pixel intensity histograms. </li>\n</font>\n</p>\n","metadata":{}},{"cell_type":"code","source":"indexes = predicted_data.sort_values('Difference', ascending = True).head(4).index.to_list()\n\ndef plotHistogram(image_index):\n\n    plt.figure(figsize = (10,7))\n    plt.subplot(2,2,1)\n    plt.imshow(x_test[image_index])\n    plt.axis('off')\n    plt.title('There is a ship. But predicted as not a ship.')\n    histo = plt.subplot(2,2,2)\n    histo.set_ylabel('Count', fontweight = \"bold\")\n    histo.set_xlabel('Pixel Intensity', fontweight = \"bold\")\n    n_bins = 30\n    plt.hist(x_test[image_index][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\n    plt.show()\n\n\n#Implementation of the function\n\nfor i in indexes:\n    plotHistogram(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"5\" face=\"Arial\">\n<h2 align=\"left\"> Implementation of Convolutional Neural Network (CNN) </h2>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"from keras import callbacks\nmodel = Sequential()\n#\nmodel.add(Conv2D(filters = 64, kernel_size = (4,4),padding = 'Same', \n                 activation ='relu', input_shape = (80,80,3)))\nmodel.add(MaxPool2D(pool_size=(5,5)))\nmodel.add(Dropout(0.25))\n#\nmodel.add(Conv2D(filters = 32, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(1,1)))\nmodel.add(Dropout(0.25))\n#\nmodel.add(Conv2D(filters = 16, kernel_size = (2,2),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(3,3), strides=(1,1)))\nmodel.add(Dropout(0.25))\n\n# Fully connected\nmodel.add(Flatten())\nmodel.add(Dense(200, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(100, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(50, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation = \"softmax\"))\n\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\nearlystopping = callbacks.EarlyStopping(monitor =\"val_loss\", \n                                        mode =\"min\", patience = 10, \n                                        restore_best_weights = True)\nhistory = model.fit(x_train, y_train, epochs = 100, validation_data=(x_val, y_val), callbacks = [earlystopping])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Data Augmentation. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"datagen = ImageDataGenerator(\n        featurewise_center=False,\n        samplewise_center=False, \n        featurewise_std_normalization=False, \n        samplewise_std_normalization=False,  \n        zca_whitening=False,\n        rotation_range=5,  \n        zoom_range = 0.1,\n        width_shift_range=0.1,  \n        height_shift_range=0.1,  \n        horizontal_flip=False, \n        vertical_flip=False)  \n\ndatagen.fit(x_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Data augmentation is a strategy that enables practitioners to significantly increase the diversity of data available for training models, without actually collecting new data. Data augmentation techniques such as cropping, padding, and horizontal flipping are commonly used to train large neural networks. </li>\n<li> Let's see how it works. I will apply little data augmentation to an image.  </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"data = x_reshaped[y==1][15]\n# expand dimension to one sample\nsamples = expand_dims(data, 0)\n# create image data augmentation generator\ndatag = ImageDataGenerator(brightness_range=[0.2,1.0],\n                          zoom_range=[0.5,1.0],\n                          horizontal_flip=True,\n                          rotation_range=90)\n# prepare iterator\nit = datag.flow(samples, batch_size=1)\n# generate samples and plot\nplt.figure(figsize = (10,10))\nfor i in range(9):\n    # define subplot\n    plt.subplot(3,3,i+1)\n    # generate batch of images\n    batch = it.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)\n    # show the figure\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now I will apply data augmentation to train data and fit again.  </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"history = model.fit(datagen.flow(x_train, y_train), epochs = 100, \n                    validation_data=(x_val, y_val), callbacks = [earlystopping])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.evaluate(x_test, y_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nimport seaborn as sns\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# Compute the confusion matrix\n\nprint(\"\\n\"\"Test Accuracy Score : \",metrics.accuracy_score(Y_true, Y_pred_classes),\"\\n\")\n\nfig, axis = plt.subplots(1, 3, figsize=(20,6))\naxis[0].plot(history.history['val_accuracy'], label='val_acc')\naxis[0].set_title(\"Validation Accuracy\")\naxis[0].set_xlabel(\"Epochs\")\naxis[0].set_ylabel(\"Val. Acc.\")\naxis[1].plot(history.history['accuracy'], label='acc')\naxis[1].set_title(\"Training Accuracy\")\naxis[1].set_xlabel(\"Epochs\")\naxis[0].set_ylabel(\"Train. Acc.\")\naxis[2].plot(history.history['val_loss'], label='val_loss')\naxis[2].set_title(\"Test Loss\")\naxis[2].set_xlabel(\"Epochs\")\naxis[2].set_ylabel(\"Loss\")\n\nplt.show()\n\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# Plot the confusion matrix\nf,ax = plt.subplots(figsize=(7, 7))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01,linecolor=\"gray\", fmt= '.1f',ax=ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.tight_layout()\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(history.history).plot();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now we can predict our test data with CNN. I check the first image of the test data.  </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"prediction = model.predict(x_test)\npd.Series(prediction[0], index=[\"Not A Ship\", \"Ship\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> You can see that we get a result with a much higher probability than the model we created with ANN. </li>\n<li> Let's take a look at the images that are estimated to be ship, although not a ship. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"predicted_data = pd.DataFrame(prediction, columns=[\"Not A Ship\", \"Ship\"])\npredicted_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test_data = pd.DataFrame(y_test, columns=[\"Not A Ship\", \"Ship\"])\ny_test_data.head(3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_data['There is a Ship'] = y_test[:, 1]\npredicted_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_data[\"Difference\"] = predicted_data[\"Ship\"] - predicted_data[\"There is a Ship\"]\npredicted_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> If difference is high, that means the image predicted as Ship, altought not a ship. To see such predicted images, we need to sort the difference column from largest to smallest. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"predicted_data.sort_values('Difference', ascending=False).head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> Now, I want to show some of these images and their pixel intensity histograms. </li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"indexes = predicted_data.sort_values('Difference', ascending = False).head(4).index.to_list()\n\ndef plotHistogram(image_index):\n\n    plt.figure(figsize = (10,7))\n    plt.subplot(2,2,1)\n    plt.imshow(x_test[image_index])\n    plt.axis('off')\n    plt.title('There is no ship. But predicted as a ship.')\n    histo = plt.subplot(2,2,2)\n    histo.set_ylabel('Count', fontweight = \"bold\")\n    histo.set_xlabel('Pixel Intensity', fontweight = \"bold\")\n    n_bins = 30\n    plt.hist(x_test[image_index][:,:,0].flatten(), bins = n_bins, lw = 0, color = 'r', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,1].flatten(), bins = n_bins, lw = 0, color = 'g', alpha = 0.5);\n    plt.hist(x_test[image_index][:,:,2].flatten(), bins = n_bins, lw = 0, color = 'b', alpha = 0.5);\n    plt.show()\n\n\n#Implementation of the function\n\nfor i in indexes:\n    plotHistogram(i)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p>\n<font size=\"4\" face=\"Arial\">\n<li> In conclusion, we achieve the best test accuracy with Convolutional Neurol Network.</li>\n</font>\n</p>","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}